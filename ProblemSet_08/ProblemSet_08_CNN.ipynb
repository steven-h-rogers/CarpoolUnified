{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steven-h-rogers/CarpoolUnified/blob/main/ProblemSet_08/ProblemSet_08_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoKGHVI2gK_E"
      },
      "source": [
        "This notebook is part the of Dr. Christoforos Christoforou's course materials. You may not, nor may you knowingly allow others to reproduce or distribute lecture notes, course materials or any of their derivatives without the instructor's express written consent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjBbvzidgTcG"
      },
      "source": [
        "# Setting up the environment\n",
        "import tensorflow.keras as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrGdpknfhmeZ"
      },
      "source": [
        "## Exercise 1: Load and preprocess the dataset.\n",
        "\n",
        "**Task 1: Load the MNIST dataset**: In the cell below, load the MNIST dataset, and standardize the dataset by applying the following pre-processing steps:\n",
        "\n",
        "- Reshape the data array to `channels_last` (i.e. `(obs,row,cols,color)`).\n",
        "- Convert the data array to `float32`.\n",
        "- Normalize data array (i.e. divide by 255)\n",
        "- Convert labels to categorical variables (i.e. one-hot encoding) using the `tensorflow.keras.utils.to_categorical(y_train)`.\n",
        "\n",
        "At the completion of this step you should have the following variables:\n",
        "Shape X_train : (60000, 28, 28, 1)\n",
        "Shape y_train : (60000, 10)\n",
        "Shape X_test : (10000, 28, 28, 1)\n",
        "Shape y_test : (10000, 10)\n",
        "\n",
        "* `X_train`,`X_test`: 4D array of shape (60000, 28, 28, 1) and (10000, 28, 28, 1) respectively, that stores the standardized MNIST data.\n",
        "* `y_train` and `y_test`: 2D array of shape (60000, 10) and (10000, 10) which uses one-hot-encoding for the labels in the MNIST dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZBcDWg6gn5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e026572e-eeb9-4a06-fcfc-0ab5d39e541c"
      },
      "source": [
        "#\n",
        "# Implement task 1 on this cell.\n",
        "#\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "(Nobs_train, n_rows, n_cols) = X_train.shape\n",
        "\n",
        "# Reshpep array to (obs,row,cols,color); convert to float and normalize\n",
        "X_train = X_train.reshape(Nobs_train,n_rows,n_cols,1).astype('float32')/255\n",
        "\n",
        "# Convert y_train labels to one-hot encoding\n",
        "y_train = tensorflow.keras.utils.to_categorical(y_train)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUjRUwoDmeFy"
      },
      "source": [
        "## Exercise 2:Building a Convolutional Neural Network using high-level Keras API\n",
        "\n",
        "In this introductory example, we will build a convolutional neural network using high-level Keras API. For that, you will need several objects provided by the API; which include the `Sequential` model which is defined under the module `tensorflow.keras.models`, and four computational layers that are defined under the `tensorflow.keras.layers module`. These layers include\n",
        "\n",
        "* the `Conv2D` layer which implement the convolutional layer;\n",
        "* the `MaxPool2D` layer which implements the max pooling operation;\n",
        "* the `Flatten` layer and the `Dense` layer\n",
        "\n",
        "These libraries have been imported at the beginning of the notebook, but to make this section self-contained, we re-import them in the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgZqFARQrLVu"
      },
      "source": [
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
        "import time\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o1pmE4UrNT8"
      },
      "source": [
        "**Defining a CNN Architecture**\n",
        "\n",
        "We can define the convolutional neural network architecture using the keras API. First, we define a general `Sequential` model and then `add` to it the various computational layer.\n",
        "\n",
        "**Convolutional layer:** The first layer of the model will be a convolutional layer defined by the `Conv2D` layer in keras. The `Conv2D` layer takes several parameters, but in this example, we will focus on the two required parameters; these are the `filters` and the `kernel_size`. The `filters` argument is an integer that specifies the number of filters (i.e. kernels) to use used during the cross-correlation (we can think of this parameter as the size of the filter bank we want to use). The `kernel_size` parameters, is a tuple that specifies the kernel size to use. For example, the code `Conv2D(filters=32, kernel_size=(3,3))` specifies a convolutional layer that uses 32 filters/kernels, and each kernel is of size 3x3. Typically, when we define a convolutional layer on the input data, we must specify the `input_shape` parameter, which is a tuple which stores the dimension of the input tensor. For example, if our input image is of shape `(28,28,1)`, as it is the case for the MNIST dataset, we can specify that when we define the first `Conv2D` layer in our network, using the syntax `Conv2D(filters=32, kernel_size=(3,3), input_shape=(28,28,1))`. The input shape parameter is authomatically set of subsequent layers in our network.\n",
        "\n",
        "**MaxPooling layer:**After adding a convolutional layer, we can typically apply a `MaxPool` layer, to reduce the spatial resolution of the output tensor. The `MaxPool` layer takes as input the `pool_size=(2,2)`.\n",
        "\n",
        "After we specify the convolutional and max-pooling layer in our architecture, we can then append the necessary dense layer to perform prediction task. The following code illustrates how to define a simple CNN architecture using the above layers.\n",
        "\n",
        "```python\n",
        "model= Sequential()\n",
        "\n",
        "input_shape = X_train[0].shape\n",
        "\n",
        "# 32: number of filters, (3,3): kernel_size, input_shape (28,28,1)\n",
        "# Output:  #epoch x 26 x 26 x filters ; convolutions reduces image size by kernal-size-1\n",
        "# #params 3*3*32= 280 + 32 biases.\n",
        "model.add(Conv2D(32,(3,3),activation='relu',input_shape=input_shape))\n",
        "\n",
        "# MaxPool2D Layer ->\n",
        "# Output : epoch x 13 x 13 x #fitlers\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "# Dropout Layer Output epoch x 13 x 13 x #fitlers\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# parameters : 3*3*32*64=18432 for W, plus 64 biases =  18496\n",
        "model.add(Conv2D(64,(3,3),activation='relu'))\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "# Flatten tensor epoch x 5408; (13*13*32-filters)\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense Layer Output : epoch x 32\n",
        "model.add(Dense(32,activation='relu'))\n",
        "\n",
        "# Dropout layer epoch x 32\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Dense layer : expoch x 10\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "**Task 2.1** In the cell below, define the neural network architecture specified in the section above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2Layyeyv4cy"
      },
      "source": [
        "# Implement Task 2.1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ_EZE3bwCDW"
      },
      "source": [
        "**Compile the Neural Network Architecture**\n",
        "Once we define the model, we need to compile it using the `model.compile` method of the API. We can train the compiled model  by calling the `model.fit` method of the API. Once we have a trained model, we can evaluate its performance on a test set by using `model.evaluate` method and apply the model to make prediction on new unseen data using the `model.predict` method.\n",
        "\n",
        "**Task 2.2:** Compile the Neural Network Architecture you defined in task 2.1\n",
        "\n",
        "**Task 2.3** Train the model by calling the `model.fit` method\n",
        "\n",
        "**Task 2.4:** Evaluate the model you trained in task 2.3 and report its accuracy and loss.\n",
        "\n",
        "**Task 2.5** Apply the model you trained in task 2.4 on the test set and report its performance. Moreover, identify the first three instances the model misclassifies and display their image; indicating in the title the predicted value.\n",
        "\n",
        "Use the cells below to complete these tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7zDxn-wxXZ_"
      },
      "source": [
        "#\n",
        "# Complete task 2.2 here.\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FNTCJF70rkB"
      },
      "source": [
        "#\n",
        "# Complete task 2.3 here.\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgfqOlUW0tc9"
      },
      "source": [
        "#\n",
        "# Complete task 2.4 here.\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYV3mU-V0v6w"
      },
      "source": [
        "#\n",
        "# Complete task 2.5 here.\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9dyeGY_00c8"
      },
      "source": [
        "## Exercise 3 Loading your own dataset.\n",
        "\n",
        "The code below illustrates how to download a remote dataset, and unzip it. As an example, we use the Cats and Dogs dataset provided by Microsoft and is available at this url https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765  (direct link: https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip)\n",
        "\n",
        "You can download the dataset using the `wget` command; and unzip it using the `uzip` command. The code below illustrates how to do this\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlmaFfrg2km6"
      },
      "source": [
        "!wget -O /content/sample_data/CatsAndDogs.zip https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dazhBQh32vFI"
      },
      "source": [
        "!unzip /content/sample_data/CatsAndDogs.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb0JCIET20Wr"
      },
      "source": [
        "To preprocess the dataset, we need the following libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIq5j_kL042W"
      },
      "source": [
        "from skimage import io\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pItHfWjN25kf"
      },
      "source": [
        "A basic pre-processing step we need to apply in new image datasets is to resize them and normalize them. As part of this example, we will pre-process the image by cropping each image to a square; resize it to a requested size; and normalize each image. The function `standarize_image` below does exactly that. You will use this function to pre-process the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH5QrR7d1Lpu"
      },
      "source": [
        "# Takes as input an image-array, resizes the image, and normalizes it.\n",
        "def standarize_image(img_array,resize_pixels=256):\n",
        "  # Convert array to image;\n",
        "  img = Image.fromarray(img_array)\n",
        "\n",
        "  # Convert image into a square image.\n",
        "  cols, rows = img.size\n",
        "  extra = (rows-cols)/2\n",
        "\n",
        "  if (extra>0):\n",
        "    # more rows than columns, crop rows\n",
        "    crop_box = (0,extra,cols,cols+extra)\n",
        "  else:\n",
        "    # more cols than rows, crop cols\n",
        "    crop_box = (-extra,0,rows-extra,rows)\n",
        "\n",
        "  # Crop image into a square and resize image based on resize_pixels\n",
        "  standarized_image = img.crop(crop_box).resize((resize_pixels,resize_pixels), Image.ANTIALIAS)\n",
        "\n",
        "  # conver image to vector convert type to float; normalize values between -1 and 1.\n",
        "  standarized_image_vector = (np.asarray(standarized_image).flatten().astype(np.float32)-128)/128\n",
        "\n",
        "  standarized_image_tensor = (np.asarray(standarized_image).astype(np.float32)-128)/128\n",
        "\n",
        "  return standarized_image_tensor, standarized_image_vector, standarized_image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsFuO7yL3fSd"
      },
      "source": [
        "**Load and pre-process the dataset**\n",
        "The code below illustrates how to load the dataset.\n",
        "```python\n",
        "# Go over the entire dataset and convert it into a single files.\n",
        "x_all = []\n",
        "y_all = []\n",
        "\n",
        "Categories = [\"Cat\",\"Dog\"]\n",
        "sample_size = 100\n",
        "\n",
        "for category in Categories:     # do dogs and cats\n",
        "    class_num = Categories.index(category)\n",
        "    path = os.path.join(data_root_folder,category)  # create path to dogs and cats\n",
        "   \n",
        "    image_list = os.listdir(path)\n",
        "    image_list = image_list[0:sample_size]\n",
        "    for img in image_list:  # iterate over each image per dogs and cats\n",
        "      img_array = io.imread(image)  \n",
        "      (img_tensor,_,img) = standarize_image(img_array,32)\n",
        "    \n",
        "      x_all.append(img_tensor)\n",
        "      y_all.append(class_num)\n",
        "\n",
        "# Convert the list to a 4D array\n",
        "x_all = np.array(x_all)\n",
        "y_all = np.array(y_all)\n",
        "```\n",
        "**Task 3.1** Reproduce the code above, but change it so that it returns a sample of 4000 images (2000 for each class) of size 64 by 64 pixels each.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7oPavDd35KU"
      },
      "source": [
        "# Implement task 3.1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVGWBSZw4acY"
      },
      "source": [
        "In the steps above, you create a standardized  dataset `x_all` that stores all observations in the dataset, and `y_all` that stores all the corresponding elements in the dataset. To train a model, we need to `slit` the dataset into a train and test set. We can do that using the `train_test_split` method of `sklearn` library as follows :\n",
        "\n",
        "```python\n",
        "(X_train, X_test, y_train, y_test) = train_test_split(x_all, y_all, test_size=0.25, random_state=42)\n",
        "```\n",
        "then to store the dataset for further processing, we can use the `save` method provided by numpy as follows:\n",
        "\n",
        "```python\n",
        "dataset = ((X_train, y_train), (X_test, y_test))\n",
        "\n",
        "# saves data in a file CatDog_preprocessed.npy, under folder sample_data.\n",
        "np.save('sample_data/CatsDog_preprocessed',dataset)\n",
        "```\n",
        "\n",
        "Subsequently, we can load the data in a format we can use in a network; using the following code:\n",
        "```python\n",
        "(X_train, y_train), (X_test, y_test) = np.load(\"sample_data/CatsDog_preprocessed.npy\",allow_pickle=True)\n",
        "\n",
        "```\n",
        "\n",
        "**Task 3.2:** Use the sample code above to a) split the `x_all` and `y_all` dataset into a `train` and `test` set. Save the resulting slit into a tuple format (similar to the format the MNIST dataset is stored)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3b_Szi-6xxE"
      },
      "source": [
        "#\n",
        "# Task 3.2: Implement the task 3.2\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmL-545R64hd"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "Build a convolutional neural network (CNN) to classify images to either `cats` or `dogs` using the Microsoft dataset. Report the network performance and apply the network on the test set. Use as many cells as necessary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bxFJK2v7gwI"
      },
      "source": [
        "#\n",
        "# Implement exercise 4; use additional cells as necessary.\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80gXdanCgZ2v"
      },
      "source": [
        "Copyright Statement: Copyright © 2020 Christoforou. The materials provided by the instructor of this course, including this notebook, are for the use of the students enrolled in the course. Materials are presented in an educational context for personal use and study and should not be shared, distributed, disseminated or sold in print — or digitally — outside the course without permission. You may not, nor may you knowingly allow others to reproduce or distribute lecture notes, course materials as well as any of their derivatives without the instructor's express written consent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k78G2GRgcnf"
      },
      "source": [
        "Conv2D()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}